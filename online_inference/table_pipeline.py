import numpy as np
import pandas as pd
import json
import re
import torch
import torch.nn.functional as F
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import math
import re
from difflib import get_close_matches
from chat_utils import get_chat_result
from config import config_mapping
from utils.tool_utils import Embedder
import time
from contextlib import contextmanager
from collections import Counter
import math

@contextmanager
def timer(name):
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    start = time.perf_counter()
    yield
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    end = time.perf_counter()
    print(f"â±ï¸  [{name}] Time: {end - start:.4f}s")


class TableRAGPipeline:
    """
    é›†æˆäº†ï¼šè¡¨æ ¼é‡æ„ã€BGE å‘é‡æ£€ç´¢ã€Schema Pruning (åˆ—ç­›é€‰) å’Œ å­è¡¨ç”Ÿæˆã€‚
    """

    def __init__(self,
                 df: pd.DataFrame,
                 external_text_list: List[str],  # æ ¸å¿ƒæ”¹åŠ¨ï¼šç›´æ¥è¾“å…¥å­—ç¬¦ä¸²åˆ—è¡¨
                 llm_backbone: str,
                 embedder: Embedder):

        self.df = df
        self.raw_text_list = external_text_list
        # 1. åŠ è½½ LLM é…ç½®
        self.llm_config = config_mapping.get(llm_backbone)
        if not self.llm_config:
            raise ValueError(f"Backbone {llm_backbone} not found in config_mapping")

        # é¢„å¤„ç†ï¼šè½¬å­—ç¬¦ä¸²ï¼Œå¡«å……ç©ºå€¼
        self.df = self.df.astype(str).replace('nan', '')
        self.embedder = embedder

        # 4. å†…éƒ¨çŠ¶æ€å­˜å‚¨
        self.documents = []  # å­˜å‚¨è½¬åŒ–åçš„å®ä½“æ–‡æ¡£
        self.table_embeddings = None  # è¡¨æ ¼è¡Œå‘é‡ (Tensor)
        self.text_embeddings = None  # æ–‡æœ¬å—å‘é‡
        self.template = ""  # å­˜å‚¨ç”Ÿæˆçš„é€šç”¨æ¨¡æ¿
        self.pk_col = self.df.columns[0]  # é»˜è®¤ç¬¬ä¸€åˆ—ä¸ºä¸»é”®

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # self.nli_model_name = "models/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"
        # self.nli_tokenizer = AutoTokenizer.from_pretrained(self.nli_model_name)
        # self.nli_model = AutoModelForSequenceClassification.from_pretrained(self.nli_model_name).to(self.device)
        # self.nli_model.eval()  # åŠ¡å¿…å¼€å¯ eval æ¨¡å¼ï¼Œå…³é—­ Dropout
        # self.nli_labels = ["entailment", "neutral", "contradiction"]

    def _clean_json_response(self, content: str) -> Dict:
        """Helper: é²æ£’çš„ JSON æå–å™¨"""
        content = content.strip()
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        json_str = match.group(1) if match else content
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            print(f"âŒ JSON Parse Failed. Raw:\n{content}")
            return {}

    # =========================================================================
    # PHASE 1: ç¦»çº¿ç´¢å¼•æ„å»º (Offline Indexing)
    # =========================================================================

    def _generate_generic_template(self) -> Dict:
        """è®© LLM çœ‹è¡¨å¤´ï¼Œç”Ÿæˆä¸€ä¸ªé€šç”¨çš„ã€ä¸­ç«‹çš„è¡Œæè¿°æ¨¡æ¿"""
        columns = self.df.columns.tolist()
        prompt = """
You are a Data-to-Text Template Generator.
Input Columns: {columns}

Goal: Create a python format string to convert a table row into a natural language sentence.

CRITICAL RULES (Follow Strictness Level: MAX):
1. **DO NOT change column names.** Keep them EXACTLY as provided in the Input Columns.
2. **DO NOT replace spaces with underscores.**
   - WRONG: {{Software_license}}
   - CORRECT: {{Software license}}
3. Use double curly braces for placeholders: {{Column Name}}.
4. Do NOT infer or hallucinate information not present in the columns.

Output JSON only:
{{
  "primary_key": "<best identifier column>",
  "template": "<sentence template>"
}}
"""
        formatted_prompt = prompt.format(columns=', '.join(columns))
        print(f"ğŸ¤– [LLM] Generating generic row template...")
        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )
        return self._clean_json_response(response.content)

    def _smart_format(self, template: str, row_dict: Dict) -> str:
        """
        å¡«å……å™¨ï¼šå…è®¸ LLM ç¨å¾®å†™é”™åˆ—åï¼Œä»£ç è´Ÿè´£è‡ªåŠ¨çº æ­£ã€‚
        """
        # 1. æ‰¾å‡ºæ¨¡æ¿é‡Œæ‰€æœ‰éœ€è¦çš„ {Key}
        # æ¯”å¦‚æ¨¡æ¿æ˜¯ "{Browser} uses {Engine}." -> æå–å‡º ['Browser', 'Engine']
        needed_keys = re.findall(r'\{(.+?)\}', template)

        # 2. å‡†å¤‡å®é™…çš„æ•°æ®æ± 
        actual_keys = list(row_dict.keys())
        # åˆ›å»ºä¸€ä¸ªå½’ä¸€åŒ–æ˜ å°„ (å…¨å°å†™ -> çœŸå®Key)
        lower_map = {k.lower().strip(): k for k in actual_keys}

        # 3. æ„å»ºæœ€ç»ˆçš„å¡«å……å­—å…¸
        final_mapping = {}

        for placeholder in needed_keys:
            # Case A: å®Œå…¨åŒ¹é… (æœ€å®Œç¾)
            if placeholder in row_dict:
                final_mapping[placeholder] = row_dict[placeholder]
                continue

            # Case B: å¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼åŒ¹é…
            clean_placeholder = placeholder.lower().strip()
            if clean_placeholder in lower_map:
                real_key = lower_map[clean_placeholder]
                final_mapping[placeholder] = row_dict[real_key]
                continue

            # Case C: æ¨¡ç³ŠåŒ¹é… (difflib)
            # æ¯”å¦‚ LLM å†™äº† {Layout}ï¼Œå®é™…æ˜¯ {Current layout engine}
            # cutoff=0.6 è¡¨ç¤ºåªè¦æœ‰ 60% åƒå°±å¯ä»¥
            matches = get_close_matches(placeholder, actual_keys, n=1, cutoff=0.6)
            if matches:
                real_key = matches[0]
                final_mapping[placeholder] = row_dict[real_key]
                # print(f"ğŸ”§ Auto-fixed: {{{placeholder}}} -> '{real_key}'") # è°ƒè¯•ç”¨
            else:
                # Case D: å®åœ¨æ‰¾ä¸åˆ°ï¼Œå¡«ä¸ªé»˜è®¤å€¼ï¼Œä¿è¯ä¸å´©
                final_mapping[placeholder] = "Unknown"

        # 4. å®‰å…¨å¡«å……
        return template.format(**final_mapping)

    def build_index(self):
        """æ ¸å¿ƒæµç¨‹ï¼šæ‰§è¡Œç¦»çº¿å»ºåº“"""
        print("\n=== ğŸ—ï¸ Phase 1: Building Offline Index ===")

        # 1. ç”Ÿæˆæ¨¡æ¿
        template_info = self._generate_generic_template()
        self.template = template_info.get("template", "")
        self.pk_col = template_info.get("primary_key", self.df.columns[0])
        print(f"âœ… Template: {self.template}")

        # 2. è¡Œè½¬æ–‡æœ¬ (Entity Documents)
        py_template = self.template.replace("{{", "{").replace("}}", "}")
        self.documents = []
        table_texts = []
        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc="Rows to Docs"):
            row_dict = row.to_dict()
            try:
                # ä½¿ç”¨æ™ºèƒ½æ¨¡ç³Šå¡«å……ï¼Œè€Œä¸æ˜¯æ­»æ¿çš„ format
                text = self._smart_format(py_template, row_dict)
                self.documents.append({
                    "row_id": idx,
                    "text": text,
                    "entity": row_dict.get(self.pk_col, "Unknown")
                })
                table_texts.append(text)
            except Exception:
                continue

        # 3. BGE å‘é‡åŒ– (Vectorization)
        print("âš¡ Encoding with BGE...")
        if not table_texts:
            raise ValueError("âŒ No texts generated from table! Check your template keys against dataframe columns.")
        raw_emb = torch.tensor(self.embedder.encode(table_texts))
        # æ‰‹åŠ¨è¿›è¡Œ L2 å½’ä¸€åŒ– (p=2, dim=1)
        self.table_embeddings = F.normalize(raw_emb, p=2, dim=1).cpu()

        # å¯¹å¤–éƒ¨æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–
        if self.raw_text_list and len(self.raw_text_list) > 0:
            print(f"âš¡ Encoding {len(self.raw_text_list)} External Text Blocks...")
            self.text_embeddings = F.normalize(torch.tensor(self.embedder.encode(self.raw_text_list)), p=2, dim=1).cpu()
        else:
            print("âš ï¸ Warning: external_text_list is empty, text indexing skipped.")

    # =========================================================================
    # æ¨ç†
    # =========================================================================

    def _get_top_k_indices(self, query_emb: torch.Tensor, embeddings: torch.Tensor, top_k: int) -> List[int]:
        """ç»Ÿä¸€æ£€ç´¢æ ¸å¿ƒï¼šå¤„ç† Query ç¼–ç ä¸ç›¸ä¼¼åº¦è®¡ç®—"""
        if embeddings is None: return []
        # è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
        scores = torch.matmul(embeddings, query_emb)
        top_results = torch.topk(scores, k=min(top_k, embeddings.shape[0]))
        return top_results.indices.tolist()

    def _filter_columns(self, question: str) -> Dict[str, Any]:
        """è®© LLM æ ¹æ®é—®é¢˜ç­›é€‰åˆ—ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¨å¤–çŸ¥è¯†"""
        all_cols = self.df.columns.tolist()
        prompt = """
You are a Table Column Selector for table question answering.

Input:
- Question: "{question}"
- Available Columns: {columns}

Goal:
Select a MINIMALLY SUFFICIENT set of columns to answer the question using ONLY the table.
"Minimally sufficient" means the chosen columns are enough to:
(A) locate the target row(s),
(B) perform any required operations (filter/sort/rank/aggregate/compare),
(C) extract the final answer value.

Critical constraints:
1) You may ONLY choose from the provided column names and MUST preserve the exact column strings.
2) Always include at least one entity identifier / primary-key-like column (e.g., name/player/id) if such a column exists.
3) If the question involves ranking or "most/second/top", include BOTH:
   - the metric column (e.g., Yards/Score/Count), AND
   - the rank column, unless you are certain rank is derived from exactly that same metric.
4) IMPORTANT: If the final answer is NOT explicitly available in the table columns,
   OR the question requires external descriptive facts,
   set "answer_in_table" to false.
   If the table alone is sufficient, set "answer_in_table" to true.
5) Notes / remarks columns:
   Columns such as "Notes", "Remarks", "Comments", or similar
   should be kept by default if present

Output JSON only:
{{
  "selected_columns": ["<exact column name>", ...],
  "answer_in_table": true/false
}}
    """
        formatted_prompt = prompt.format(question=question, columns=', '.join(all_cols))
        response = get_chat_result(
            messages=[{"role": "user", "content": formatted_prompt}],
            tools=None,
            llm_config=self.llm_config
        )

        result = self._clean_json_response(response.content)
        # 1. è·å– LLM æƒ³è¦ä¿ç•™çš„åˆ—
        selected = result.get("selected_columns", [])
        # 2. [å…³é”®ä¿®å¤] å¼ºåˆ¶æ³¨å…¥ Primary Key (self.pk_col)
        # æ— è®º LLM è§‰å¾—éœ€ä¸éœ€è¦ï¼Œç¨‹åºé€»è¾‘éœ€è¦å®ƒ
        if self.pk_col not in selected:
            # print(f"ğŸ”§ Auto-injecting PK column: {self.pk_col}")
            selected.insert(0, self.pk_col)

        # 3. æ ¡éªŒé€‰å‡ºçš„åˆ—æ˜¯å¦çœŸçš„åœ¨è¡¨ä¸­
        final_selected = [c for c in selected if c in all_cols]
        if not final_selected:
            final_selected = all_cols

        print(f"ğŸ·ï¸ answer_in_table: {result['answer_in_table']}")
        result["selected_columns"] = final_selected
        return result

    def _analyze_query_intent(self, question: str) -> str:
        """
        åˆ†æé—®é¢˜æ„å›¾ï¼šæ˜¯ç®€å•çš„æŸ¥å€¼ï¼Œè¿˜æ˜¯å¤æ‚çš„èšåˆ/æ’åº
        """
        q_lower = question.lower()

        # 1. èšåˆç±»å…³é”®è¯ (Aggregation)
        agg_keywords = ["how many", "sum", "average", "total", "percentage", "count", "amount"]
        if any(w in q_lower for w in agg_keywords):
            return "aggregation"

        # 2. æ’åº/æ¯”è¾ƒç±»å…³é”®è¯ (Ranking)
        # æ³¨æ„ï¼šåŒ…å« 'second', 'most', 'top' ç­‰
        rank_keywords = ["most", "least", "best", "worst", "top", "first", "second",
                         "third", "last", "rank", "sort", "highest", "lowest", "compare"]
        if any(w in q_lower for w in rank_keywords):
            return "ranking"

        # 3. é»˜è®¤æŸ¥å€¼ (Retrieval)
        return "retrieval"

    def _expand_context_radius(self, anchor_ids: List[int], intent: str) -> List[int]:
        """
        æ ¹æ®æ„å›¾è‡ªé€‚åº”åˆ†é…ä¸Šä¸‹æ–‡è¡Œã€‚
        intent: "retrieval" | "ranking" | "aggregation"
        """
        final_ids = set(anchor_ids)

        # === åœºæ™¯ A: ç®€å•æŸ¥å€¼ (Retrieval) ===
        # ç­–ç•¥ï¼šå…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡
        # é€»è¾‘ï¼šåŠ ä¸Šå‰åé‚»å±…ï¼Œå¸®åŠ©ç†è§£ä¸Šä¸‹æ–‡è¡”æ¥
        if intent == "retrieval":
            for rid in anchor_ids:
                if rid > 0: final_ids.add(rid - 1)
                if rid < len(self.df) - 1: final_ids.add(rid + 1)

        # === åœºæ™¯ B: æ’åæˆ–èšåˆ (Ranking / Aggregation) ===
        else:
            # 1. å¼ºåˆ¶åŠ å…¥ Top-10 è¡Œ
            top_n_count = 10
            for i in range(min(top_n_count, len(self.df))):
                final_ids.add(i)

        # === æœ€ç»ˆå¤„ç† ===
        sorted_ids = sorted(list(final_ids))
        # åŠ¨æ€æˆªæ–­ï¼šå¦‚æœæ˜¯ Ranking é—®é¢˜ï¼Œå°½é‡å¤šç»™å‡ è¡Œï¼Œé˜²æ­¢æ¦œå•æ–­è£‚
        limit = 25 if intent in ["ranking", "aggregation"] else 15

        return sorted_ids[:limit]

    def _retrieve_and_prune_text(self, query_emb: torch.Tensor, anchor_entities: List[str],
                                 retrieved_texts: List[str]) -> List[Dict]:
        """
        [Text Pruning] åŒè·¯å¬å›ç‰ˆ (Dual-Route Retrieval)
        ä¸ºäº†é˜²æ­¢åŠ æƒç­–ç•¥å¯¼è‡´çš„â€œé€†åâ€ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†è·¯å½•å–ç­–ç•¥ï¼š
        1. è¯­ä¹‰é€šé“ï¼šå½•å–å‘é‡ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æœ¬ã€‚
        2. è¯æ±‡é€šé“ï¼šå½•å–å®ä½“å…³é”®è¯åŒ¹é…åº¦æœ€é«˜çš„æ–‡æœ¬ã€‚
        æœ€åå–å¹¶é›†ã€‚
        """
        if not retrieved_texts: return []

        # --- 1. å‡†å¤‡ IDF æƒé‡ (ç”¨äºè¯æ±‡é€šé“) ---
        # ç»Ÿè®¡ anchor å®ä½“çš„è¯é¢‘ï¼Œè®¡ç®—ç®€æ˜“ IDF
        all_anchor_tokens = []
        for ent in anchor_entities:
            tokens = [w.lower() for w in re.split(r'\W+', str(ent)) if len(w) > 2]
            all_anchor_tokens.extend(tokens)

        token_counts = Counter(all_anchor_tokens)
        num_anchors = max(1, len(anchor_entities))

        idf_weights = {}
        for token, count in token_counts.items():
            # è¶Šç¨€æœ‰çš„å®ä½“è¯ï¼Œæƒé‡è¶Šå¤§
            idf_weights[token] = math.log(num_anchors / (count + 1)) + 1.0

        # --- 2. æ–‡æœ¬é¢„å¤„ç†ä¸å»é‡ ---
        seen_units = set()
        for text in retrieved_texts:
            is_kv = len(re.findall(r'[:ï¼š|]', text)) > len(text) / 50
            units = re.split(r'[\n;]', text) if is_kv else re.split(r'(?<=[ã€‚ï¼Ÿï¼?.])\s+', text)
            for u in units:
                u_clean = u.strip()
                if len(u_clean) > 5 and u_clean not in seen_units:
                    seen_units.add(u_clean)

        unique_units = list(seen_units)
        if not unique_units: return []

        # --- 3. å‘é‡åŒ– (è¯­ä¹‰é€šé“) ---
        raw_embs = torch.tensor(self.embedder.encode(unique_units))
        unit_embs = torch.nn.functional.normalize(raw_embs, p=2, dim=1)

        if query_emb.dim() == 1:
            dense_scores = torch.matmul(unit_embs, query_emb).cpu().numpy()
        else:
            dense_scores = torch.matmul(unit_embs, query_emb.t()).squeeze().cpu().numpy()

        # --- 4. è¯„åˆ†è®¡ç®— ---
        scored_units = []
        for i, text_unit in enumerate(unique_units):
            d_score = dense_scores[i]  # è¯­ä¹‰åˆ†
            text_lower = text_unit.lower()

            # è®¡ç®—è¯æ±‡åˆ† (Lexical Score)
            s_score = 0.0
            for token, weight in idf_weights.items():
                if token in text_lower:
                    s_score += weight

            scored_units.append({
                "text": text_unit,
                "embedding": unit_embs[i],
                "dense_score": d_score,
                "sparse_score": s_score,
                "original_index": i
            })

        # --- 5. åŒè·¯å½•å– (Dual Selection) ---

        # é¢„ç®—åˆ†é…ï¼šæ€»å…±ç•™ 35~50 ä¸ª
        # è¯­ä¹‰é€šé“å  70%ï¼Œè¯æ±‡é€šé“å  30% (ä¿è¯è¯­ä¹‰æ˜¯ä¸»æµï¼Œå…³é”®è¯æ˜¯è¡¥å……)
        total_budget = min(50, math.ceil(len(scored_units) * 0.6))
        total_budget = max(25, total_budget)  # è‡³å°‘ç•™ 25 ä¸ª

        semantic_budget = int(total_budget * 0.7)
        lexical_budget = total_budget - semantic_budget

        final_indices = set()

        # Route A: è¯­ä¹‰ä¼˜å…ˆ (Vector High Score)
        scored_units.sort(key=lambda x: x["dense_score"], reverse=True)
        for i in range(min(len(scored_units), semantic_budget)):
            final_indices.add(scored_units[i]["original_index"])

        # Route B: è¯æ±‡ä¼˜å…ˆ (Keyword High Score)
        # é‡æ–°æ’åºï¼Œè¿™æ¬¡çœ‹ sparse_score
        scored_units.sort(key=lambda x: x["sparse_score"], reverse=True)

        # å½•å–é‚£äº›è¿˜æ²¡æœ‰è¢«è¯­ä¹‰é€šé“é€‰ä¸­çš„â€œæ¼ç½‘ä¹‹é±¼â€
        added_count = 0
        for unit in scored_units:
            if added_count >= lexical_budget:
                break
            if unit["original_index"] not in final_indices:
                # åªæœ‰å½“å®ƒç¡®å®åŒ…å«å…³é”®è¯ (sparse_score > 0) æ—¶æ‰æ•‘å›
                if unit["sparse_score"] > 0:
                    final_indices.add(unit["original_index"])
                    added_count += 1

        # --- 6. ç»„è£…æœ€ç»ˆç»“æœ ---
        # æŒ‰ç…§åŸå§‹çš„è¯­ä¹‰åˆ†æ•°æ’åºè¾“å‡ºï¼Œä¿è¯åç»­å¤„ç†é¡ºåºæ­£å¸¸
        final_result = []
        for i in range(len(unique_units)):
            if i in final_indices:
                # æ‰¾åˆ°å¯¹åº”çš„åˆ†æ•°å¯¹è±¡
                # ä¸ºäº†åç»­å…¼å®¹ï¼Œæˆ‘ä»¬æŠŠ score è®¾ä¸º dense_scoreï¼Œå› ä¸ºå¯¹é½é˜¶æ®µä¼šé‡æ–°ç®—
                unit_obj = next(u for u in scored_units if u["original_index"] == i)
                final_result.append({
                    "text": unit_obj["text"],
                    "score": unit_obj["dense_score"],  # ä¿æŒ API å…¼å®¹
                    "embedding": unit_obj["embedding"]
                })

        # å†æ¬¡æŒ‰åˆ†æ•°æ’åºè¿”å›
        final_result.sort(key=lambda x: x["score"], reverse=True)
        return final_result

    # def _retrieve_and_prune_text(self, query_emb: torch.Tensor, anchor_entities: List[str],
    #                              retrieved_texts: List[str]) -> List[Dict]:
    #     """
    #     2. è‡ªåŠ¨åˆ¤å®š KV ç»“æ„ä¸å¥å­ç»“æ„
    #     3. åŸºäº BGE ç›¸ä¼¼åº¦ä¸å®ä½“é”šå®šæ‰“åˆ†
    #     """
    #     if not retrieved_texts: return []
    #
    #     entity_keywords = set()
    #     for ent in anchor_entities:
    #         for word in re.split(r'\W+', ent):  # æŒ‰éå­—æ¯å­—ç¬¦æ‹†åˆ†
    #             if len(word) > 3:  entity_keywords.add(word.lower())
    #
    #     seen_units = set()  # ç”¨äºå»é‡
    #     for text in retrieved_texts:
    #         # è‡ªåŠ¨åˆ¤å®š KV vs çº¯æ–‡æœ¬ç»“æ„
    #         is_kv = len(re.findall(r'[:ï¼š|]', text)) > len(text) / 50
    #         units = re.split(r'[\n;]', text) if is_kv else re.split(r'(?<=[ã€‚ï¼Ÿï¼?.])\s+', text)
    #         for u in units:
    #             u_clean = u.strip()
    #             if len(u_clean) > 5 and u_clean not in seen_units:
    #                 seen_units.add(u_clean)
    #
    #     unique_units = list(seen_units)
    #     if not seen_units: return []
    #
    #     # å‘é‡åŒ– (å¢åŠ æ‰‹åŠ¨å½’ä¸€åŒ–ï¼Œç¡®ä¿åç»­è®¡ç®—å‡†ç¡®)
    #     # raw_embs: [N, Dim]
    #     raw_embs = torch.tensor(self.embedder.encode(unique_units))
    #     unit_embs = torch.nn.functional.normalize(raw_embs, p=2, dim=1)
    #     # æ‰“åˆ† (Query vs Units)
    #     if query_emb.dim() == 1:
    #         scores = torch.matmul(unit_embs, query_emb)
    #     else:
    #         scores = torch.matmul(unit_embs, query_emb.t()).squeeze()
    #     scores = scores.cpu().numpy()
    #
    #     all_units = []
    #     for i, score in enumerate(scores):
    #         text_unit = unique_units[i]
    #         # å…³é”®è¯åŠ åˆ†
    #         if any(kw in text_unit.lower() for kw in entity_keywords):
    #             score += 0.2
    #         all_units.append({
    #             "text": text_unit,
    #             "score": score,
    #             "embedding": unit_embs[i]  # å¸¦å‡ºå‘é‡ï¼Œä¾›ä¸‹ä¸€æ­¥å¯¹é½ä½¿ç”¨
    #         })
    #
    #     # ä¿ç•™å‰ 50%
    #     all_units.sort(key=lambda x: x["score"], reverse=True)
    #     keep_count = min(80, math.ceil(len(all_units) * 0.5))  # ç¨å¾®æ”¾å®½ä¸€ç‚¹ä¸Šé™åˆ°ï¼Œä¿è¯ä¸Šä¸‹æ–‡
    #
    #     return all_units[:keep_count]

    def _inject_cross_references(self, sub_df: pd.DataFrame, pruned_units: List[Dict]) -> Dict[str, str]:
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šé€šç”¨æ··åˆæ£€ç´¢å¯¹é½ (Robust Hybrid Alignment)
        ä¸å†ä½¿ç”¨ç¡¬é˜ˆå€¼ä¿é€ï¼Œè€Œæ˜¯ä½¿ç”¨åŠ æƒèåˆã€‚å¼•å…¥ IDF æ€æƒ³,å…³é”®è¯åŒ¹é…ä¸èƒ½â€œå‘½ä¸­ä¸€ä¸ªå°±ç»™æ»¡åˆ†â€ã€‚å‘½ä¸­ç¨€æœ‰è¯ï¼ˆå¦‚ "Android"ï¼‰ç»™é«˜åˆ†ï¼Œå‘½ä¸­æ™®é€šè¯ç»™ä½åˆ†ã€‚
        """
        if not pruned_units:
            return {"table_md": sub_df.to_markdown(index=False), "text_str": ""}

        # 1. [æ–°å¢] åŠ¨æ€è®¡ç®—è¡¨æ ¼å†…çš„ IDF (è¯çš„ç¨€ç¼ºåº¦)
        all_tokens_flat = []
        for val in sub_df[self.pk_col]:
            # ç®€å•åˆ†è¯ï¼Œè¿‡æ»¤çŸ­è¯
            tokens = [w.lower() for w in re.split(r'\W+', str(val)) if len(w) > 2]
            all_tokens_flat.extend(tokens)

        token_counts = Counter(all_tokens_flat)
        total_rows = len(sub_df)

        # è®¡ç®—æ¯ä¸ªè¯çš„ IDF æƒé‡: log(æ€»è¡Œæ•° / (è¯é¢‘ + 1)) + 1
        # ç¨€æœ‰è¯æƒé‡é«˜ï¼Œé«˜é¢‘è¯(å¦‚ Browser)æƒé‡ä½
        idf_weights = {}
        for token, count in token_counts.items():
            idf_weights[token] = math.log(total_rows / (count + 1)) + 1.0

        # 2. å‡†å¤‡å‘é‡
        # ç¡®ä¿éƒ½åœ¨ CPU ä¸Šè®¡ç®—
        unit_embs = torch.stack([u['embedding'] for u in pruned_units]).cpu()
        row_indices = sub_df.index.tolist()
        row_embs = self.table_embeddings[row_indices].cpu()

        # [K, M] å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µ
        dense_scores = torch.matmul(row_embs, unit_embs.t()).numpy()

        # 3. å®¹å™¨
        row_refs = {i: [] for i in range(len(sub_df))}
        unit_labels = {j: set() for j in range(len(pruned_units))}

        # 4. æ··åˆæ£€ç´¢å¾ªç¯
        for r_idx in range(len(sub_df)):
            row_entity = str(sub_df.iloc[r_idx][self.pk_col])
            # æå–å½“å‰è¡Œçš„å®ä½“ tokens
            row_tokens = [w.lower() for w in re.split(r'\W+', row_entity) if len(w) > 2]

            candidates = []

            for u_idx in range(len(pruned_units)):
                # A. ç¨ å¯†åˆ† (Dense Score): èŒƒå›´é€šå¸¸ -1 ~ 1
                d_score = dense_scores[r_idx][u_idx]

                # B. ç¨€ç–åˆ† (Sparse Score): åŸºäº IDF åŠ æƒ
                text_content = pruned_units[u_idx]['text'].lower()

                s_score = 0.0
                for token in row_tokens:
                    if token in text_content:
                        # å‘½ä¸­ç¨€æœ‰è¯åŠ åˆ†å¤šï¼Œå‘½ä¸­é«˜é¢‘è¯åŠ åˆ†å°‘
                        s_score += idf_weights.get(token, 1.0)

                # å½’ä¸€åŒ– Sparse Score (é˜²æ­¢é•¿å®ä½“åˆ†æ•°æ— é™è†¨èƒ€)
                # å‡è®¾åŒ¹é…äº† 2-3 ä¸ªæ ¸å¿ƒè¯å°±ç®—å¾ˆé«˜äº†ï¼Œå°é¡¶ 1.0
                s_score = min(s_score / 4.0, 1.0)

                # C. èåˆåˆ† (Hybrid Score)
                # 0.7 * å‘é‡ + 0.3 * å…³é”®è¯
                final_score = 0.7 * d_score + 0.3 * s_score

                # [æ ¸å¿ƒä¿®å¤] è¿™é‡Œå¿…é¡» Append 3ä¸ªå€¼ï¼Œå¯¹åº”åé¢è§£åŒ…çš„ 3ä¸ªå˜é‡
                candidates.append((final_score, u_idx, d_score))

            # 5. æ’åºä¸æˆªæ–­
            # æŒ‰ final_score é™åºæ’åˆ—
            candidates.sort(key=lambda x: x[0], reverse=True)
            top_k = candidates[:5]

            # 6. æœ€ç»ˆå®‰å…¨ç½‘ (Soft Threshold)
            # è¿™é‡Œè§£åŒ… 3 ä¸ªå€¼å°±ä¸ä¼šæŠ¥é”™äº†
            for f_score, u_idx, raw_score in top_k:
                # åªè¦æ··åˆåˆ† > 0.45 å°±å¯ä»¥å…¥é€‰
                # æˆ–è€…ï¼šè™½ç„¶æ··åˆåˆ†ç•¥ä½ï¼Œä½†åŸå§‹å‘é‡åˆ†æé«˜ (>0.65) ä¹Ÿå¯ä»¥å…¥é€‰
                if f_score > 0.45 or raw_score > 0.65:
                    # è®°å½•æ—¶å±•ç¤ºæ··åˆåˆ†æ•°ï¼Œæ–¹ä¾¿è°ƒè¯•
                    row_refs[r_idx].append(f"[{u_idx}]({f_score:.2f})")
                    unit_labels[u_idx].add(row_entity)

        # 7. ç”Ÿæˆå¢å¼ºç‰ˆè¡¨æ ¼
        view_df = sub_df.copy()
        view_df["Related Context IDs"] = [", ".join(refs) for refs in row_refs.values()]
        table_md = view_df.to_markdown(index=False)

        # 8. ç”Ÿæˆå¢å¼ºç‰ˆæ–‡æœ¬ä¸²
        formatted_texts = []
        for i, unit in enumerate(pruned_units):
            labels = sorted(list(unit_labels[i]))
            label_str = f"[Rel: {', '.join(labels)}]" if labels else ""
            formatted_texts.append(f"[{i}] {label_str} {unit['text']}")

        return {
            "table_md": table_md,
            "text_str": "\n".join(formatted_texts)
        }

    # def _verify_evidence(self, sub_table_facts: List[str], text_evidence: str) -> List[str]:
    #     """
    #     åˆ©ç”¨ Tokenizer çš„ Batch å¤„ç†èƒ½åŠ›ï¼Œä¸€æ¬¡æ€§æ ¡éªŒæ‰€æœ‰è¡¨æ ¼äº‹å®
    #     """
    #     if not text_evidence or not sub_table_facts:
    #         return []
    #
    #     verification_signals = []
    #     # å°†æ–‡æœ¬è¯æ®ä½œä¸ºç»Ÿä¸€çš„å‰æ (Premise)
    #     premise = text_evidence[:1500]
    #
    #     try:
    #         entail_idx = self.nli_labels.index("entailment")
    #         contra_idx = self.nli_labels.index("contradiction")
    #     except ValueError:
    #         # å…œåº•é€»è¾‘ï¼šå¦‚æœ labels è®¾ç½®ä¸å¯¹ï¼Œé»˜è®¤ä½¿ç”¨å®˜æ–¹æ ‡å‡† 0, 2
    #         entail_idx, contra_idx = 0, 2
    #
    #     # 1. æ„é€  Batch è¾“å…¥å¯¹ï¼š[[Premise, Hypo1], [Premise, Hypo2], ...]
    #     pairs = [[premise, fact] for fact in sub_table_facts]
    #
    #     # 2. è°ƒç”¨ Tokenizer çš„æ‰¹å¤„ç†åŠŸèƒ½
    #     # padding=True ä¼šè‡ªåŠ¨å¯¹é½é•¿åº¦ï¼Œreturn_tensors="pt" è¿”å› PyTorch å¼ é‡
    #     inputs = self.nli_tokenizer(
    #         pairs,
    #         padding=True,
    #         truncation=True,
    #         max_length=512,
    #         return_tensors="pt"
    #     ).to(self.device)
    #
    #     # 3. å¼€å¯æ— æ¢¯åº¦æ¨ç†æ¨¡å¼
    #     with torch.no_grad():
    #         outputs = self.nli_model(**inputs)
    #         # å¯¹ logits åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆæ ‡ç­¾ç»´åº¦ï¼‰åš Softmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ [Batch_size, 3]
    #         predictions = torch.softmax(outputs.logits, dim=-1)
    #
    #     # 4. è§£æç»“æœ (å¯¹åº”å®˜æ–¹æ ‡ç­¾é¡ºåº: entailment, neutral, contradiction)
    #     # å°†ç»“æœè½¬å› CPU åˆ—è¡¨å¤„ç†
    #     predictions = predictions.cpu().numpy()
    #
    #     for i, probs in enumerate(predictions):
    #         fact = sub_table_facts[i]
    #         entail_prob = probs[entail_idx]
    #         contra_prob = probs[contra_idx]
    #
    #         # é˜ˆå€¼åˆ¤å®šï¼šåªæœ‰ç½®ä¿¡åº¦å¤Ÿé«˜æ‰è¾“å‡ºä¿¡å·ï¼Œå‡å°‘å™ªå£°
    #         if entail_prob > 0.7:
    #             verification_signals.append(f"âœ… Fact Verified: {fact[:60]}... (Conf: {entail_prob:.1%})")
    #         elif contra_prob > 0.7:
    #             verification_signals.append(f"âŒ Conflict Detected: {fact[:60]}... (Conf: {contra_prob:.1%})")
    #
    #     return verification_signals

    def retrieve_aligned_context(self, question: str):
        """
        æ¨ç†å…¥å£ï¼šç»“åˆè‡ªé€‚åº”å­è¡¨ä¸ç²¾ç®€ KV æ–‡æœ¬
        """
        print(f"\n=== ğŸš€ Hybrid Query: {question} ===")
        query_emb_numpy = self.embedder.encode(question)
        query_emb = torch.tensor(query_emb_numpy).squeeze().cpu()

        # 1. æ„å›¾åˆ†æä¸é”šç‚¹æ£€ç´¢
        intent = self._analyze_query_intent(question)
        anchor_ids = self._get_top_k_indices(query_emb, self.table_embeddings, top_k=10)
        anchor_entities = [self.df.iloc[rid][self.pk_col] for rid in anchor_ids]
        expanded_ids = self._expand_context_radius(anchor_ids, intent)

        # å…³é”®è¯æ£€æµ‹ï¼šå¦‚æœé—®é¢˜åŒ…å«æ’åºã€æœ€å€¼ã€è®¡æ•°ç­‰è¯æ±‡
        ranking_keywords = ["most", "least", "best", "worst", "top", "first", "second", "third", "last", "rank", "sort",
                            "highest", "lowest"]
        is_ranking_query = any(kw in question.lower() for kw in ranking_keywords)

        if is_ranking_query or intent == "ranking" or intent == "aggregation":
            print(f"ğŸ“Š Detected Ranking/Aggregation Query: Preserving Table Structure...")
            # ç­–ç•¥ Aï¼šå¦‚æœæ˜¯å°è¡¨ (50è¡Œä»¥å†…)ï¼Œå¹²è„†å…¨ç»™ï¼Œä¸è¦è®© LLM çŒœ
            if len(self.df) <= 50:
                expanded_ids = list(range(len(self.df)))
            # ç­–ç•¥ Bï¼šå¦‚æœæ˜¯å¤§è¡¨ï¼Œå¼ºåˆ¶é’‰æ­»å‰ 10 è¡Œ (Pin Top-N)
            # è¿™æ · LLM å°±èƒ½çœ‹åˆ° Rank 1, 2, 3... ä»è€Œå»ºç«‹æ­£ç¡®çš„åæ ‡ç³»
            else:
                top_rows_count = 10
                # ç¡®ä¿ä¸è¶…è¿‡è¡¨é•¿åº¦
                top_ids = [i for i in range(min(top_rows_count, len(self.df)))]
                # åˆå¹¶ è¯­ä¹‰æ£€ç´¢è¡Œ + å¤´éƒ¨è¡Œ
                expanded_ids = sorted(list(set(expanded_ids + top_ids)))
        expanded_ids.sort()

        # 3.  æ„å»ºç²¾ç®€å­è¡¨
        col_info = self._filter_columns(question)
        is_sufficient = col_info.get('answer_in_table', False)  # è·å–è¿™ä¸ªå…³é”®ä¿¡å·
        # å¦‚æœè¡¨é‡Œæ²¡ç­”æ¡ˆï¼Œå°±å¼ºåˆ¶å‘½ä»¤å®ƒå»æŒ–æ–‡æœ¬
        if not is_sufficient:
            guidance = "**CRITICAL**: The Table is KNOWN to lack the specific answer. You MUST extract the answer from the Textual Evidence."
        else:
            guidance = "**Note**: The Table likely contains the answer. Verify it against the Textual Evidence."

        # è·å–åŸºç¡€å­è¡¨æ•°æ®
        subtable_df = self.df.loc[expanded_ids, col_info["selected_columns"]]
        # æ–‡æœ¬æ£€ç´¢ä¸åŒå‘æ³¨å…¥
        pruned_text_str = ""

        if self.text_embeddings is not None:
            top_text_ids = self._get_top_k_indices(query_emb, self.text_embeddings, top_k=30)
            candidate_texts = [self.raw_text_list[i] for i in top_text_ids]
            # äº¤ç»™ pruning å‡½æ•°åšæœ€åçš„å†…å®¹ç²¾ç®€
            pruned_units = self._retrieve_and_prune_text(query_emb, anchor_entities, candidate_texts)

            # æ³¨å…¥å¼•ç”¨ä¿¡æ¯,åˆ©ç”¨ä¸Šä¸€æ­¥çš„å‘é‡åšè¡¨æ–‡å¯¹é½
            injection_result = self._inject_cross_references(subtable_df, pruned_units)
            final_table_md = injection_result["table_md"]
            pruned_text_str = injection_result["text_str"]
        else:
            final_table_md = subtable_df.to_markdown(index=False)

        # 6. NLI æ ¡éªŒä¸æ˜¾å¼æ‰“å°
        # relevant_docs = [d['text'] for d in self.documents if d['row_id'] in expanded_ids]
        # nli_signals = self._verify_evidence(relevant_docs, pruned_text_str)
        # if nli_signals:
        #     print(f"\nğŸ§  [NLI Logic Check] Found {len(nli_signals)} signals:")
        #     for s in nli_signals:
        #         print(f"  - {s}")
        # else:
        #     print("\nğŸ§  [NLI Logic Check] No strong entailment or contradiction found.")

        return guidance, final_table_md, pruned_text_str

    # =========================================================================
    # æœ€ç»ˆèåˆæ¨ç† (Hybrid Inference)
    # =========================================================================
    def query(self, question: str) -> str:
        """
        æ¨ç†å…¥å£
        """
        guidance, final_table_md, pruned_text_str = self.retrieve_aligned_context(question)

        # 7. ç”Ÿæˆ
        final_prompt = f"""
    You are a factual reasoning assistant. Answer the question based on the evidence provided below.
    Rules:
1. **Check Table Sufficiency**: {guidance}

    ### 1. Structured Table Evidence (Key Rows & Columns)
    {final_table_md}
    ### 2. Supporting Textual Evidence (Extracted Facts)
    {pruned_text_str}
    - Question: {question}
    
Please format your output EXACTLY as follows:
{{
<Answer>: [The direct answer]
}}
    """

        print("\nğŸ“ [Final Prompt Context Preview]:")
        print(f"--- Table ---\n{final_table_md}\n--- Text ---\n{pruned_text_str}\n")

        # 4. ç”Ÿæˆç­”æ¡ˆ
        response = get_chat_result(
            messages=[{"role": "user", "content": final_prompt}],
            llm_config=self.llm_config
        )

        return response.content
